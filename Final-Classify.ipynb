{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**\n",
    "\n",
    "Now that the fake reviews have been generated using the re-trained gpt2 model, its time to prepare the datasets for final use within the classification model; this is done through standardizing the data and then tokenisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = '/tmp/tfhub'\n",
    "\n",
    "#Load datasets\n",
    "fake_reviews = pd.read_csv('reviews_generated.csv',usecols=['text'])\n",
    "real_reviews = pd.read_csv('bigreviews.csv',usecols=['text'])\n",
    "\n",
    "#Add new column indicating real or fake\n",
    "#real = 1 / fake = 0\n",
    "real_reviews['real'] = 1\n",
    "fake_reviews['real'] = 0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Rate of 33% of reviews suspected to be fake, so dataset will be made with this concept in mind*\n",
    "\n",
    "Generated = 7252 \n",
    "\n",
    "Real = 14508\n",
    "\n",
    "Total = 21756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the last 21756 reviews\n",
    "real_reviews = real_reviews.tail(14504)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join databases\n",
    "full_reviews = pd.concat([real_reviews, fake_reviews], ignore_index=True)\n",
    "full_reviews.to_csv('full_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  real\n",
      "0      My MacBook Pro retina was failing do the stupi...     1\n",
      "1      My boyfriend and I found this place doing a lo...     1\n",
      "2      Hubby and I decided to try.  Never been to Ger...     1\n",
      "3      Ok so this is really Aneu! I really don't know...     1\n",
      "4      Finally got to try Smee's recently.  I like th...     1\n",
      "...                                                  ...   ...\n",
      "21751  we were looking for a place to eat and we foun...     0\n",
      "21752  second time here.  the food is good, but the s...     0\n",
      "21753  tucked on 76, it's a great place to go to for ...     0\n",
      "21754  these hand grenades are the best! \\n\\nthe staf...     0\n",
      "21755  this is totallly a great place to go for a cas...     0\n",
      "\n",
      "[21756 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(full_reviews)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Clean the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reviews = pd.read_csv('full_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization and spell check\n",
    "import itertools\n",
    "import re\n",
    "from autocorrect import Speller\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def correct_text(text):\n",
    "    #One letter in a word should not be present more than twice in continuation\n",
    "    text_correction = ''.join(''.join(s)[:3] for _, s in itertools.groupby(text))\n",
    "    #Apply autocorrection to the corrected text\n",
    "    spell = Speller(lang='en')\n",
    "    ans = spell(text_correction)\n",
    "    return ans\n",
    "\n",
    "\n",
    "def standardize_text(text):\n",
    "    #Remove unicode characters\n",
    "    text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "    #Turn to lower case\n",
    "    text = text.lower()\n",
    "    #Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    #Remove punctuation\n",
    "    text = re.sub(\"[^-9A-Za-z ]\", \"\" , text)\n",
    "    #Remove double spaces\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "#Implement lemmatization, group words by root stem but keep the different tenses \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemm_text(text):\n",
    "    ans = lemmatizer.lemmatize(text)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reviews['text'] = full_reviews['text'].apply(correct_text)\n",
    "full_reviews['text'] = full_reviews['text'].apply(standardize_text)\n",
    "full_reviews['text'] = full_reviews['text'].apply(lemm_text)\n",
    "\n",
    "full_reviews.to_csv('full_reviews_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reviews"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
