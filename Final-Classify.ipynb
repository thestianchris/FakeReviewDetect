{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**\n",
    "\n",
    "Now that the fake reviews have been generated using the re-trained gpt2 model, its time to prepare the datasets for final use within the classification model; this is done through standardizing the data and then tokenisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = '/tmp/tfhub'\n",
    "\n",
    "#Load datasets\n",
    "fake_reviews = pd.read_csv('reviews_generated.csv',usecols=['text'])\n",
    "real_reviews = pd.read_csv('bigreviews.csv',usecols=['text'])\n",
    "\n",
    "#Add new column indicating real or fake\n",
    "#real = 1 / fake = 0\n",
    "real_reviews['real'] = 1\n",
    "fake_reviews['real'] = 0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Rate of 33% of reviews suspected to be fake, so dataset will be made with this concept in mind*\n",
    "\n",
    "Generated = 7252 \n",
    "\n",
    "Real = 14508\n",
    "\n",
    "Total = 21756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the last 21756 reviews\n",
    "real_reviews = real_reviews.tail(14504)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join databases\n",
    "full_reviews = pd.concat([real_reviews, fake_reviews], ignore_index=True)\n",
    "full_reviews.to_csv('full_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  real\n",
      "0      My MacBook Pro retina was failing do the stupi...     1\n",
      "1      My boyfriend and I found this place doing a lo...     1\n",
      "2      Hubby and I decided to try.  Never been to Ger...     1\n",
      "3      Ok so this is really Aneu! I really don't know...     1\n",
      "4      Finally got to try Smee's recently.  I like th...     1\n",
      "...                                                  ...   ...\n",
      "21751  we were looking for a place to eat and we foun...     0\n",
      "21752  second time here.  the food is good, but the s...     0\n",
      "21753  tucked on 76, it's a great place to go to for ...     0\n",
      "21754  these hand grenades are the best! \\n\\nthe staf...     0\n",
      "21755  this is totallly a great place to go for a cas...     0\n",
      "\n",
      "[21756 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(full_reviews)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Clean the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reviews = pd.read_csv('full_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization and spell check\n",
    "import itertools\n",
    "import re\n",
    "from autocorrect import Speller\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def correct_text(text):\n",
    "    #One letter in a word should not be present more than twice in continuation\n",
    "    text_correction = ''.join(''.join(s)[:3] for _, s in itertools.groupby(text))\n",
    "    #Apply autocorrection to the corrected text\n",
    "    spell = Speller(lang='en')\n",
    "    ans = spell(text_correction)\n",
    "    return ans\n",
    "\n",
    "\n",
    "def standardize_text(text):\n",
    "    #Remove unicode characters\n",
    "    text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "    #Turn to lower case\n",
    "    text = text.lower()\n",
    "    #Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    #Remove punctuation\n",
    "    text = re.sub(\"[^-9A-Za-z ]\", \"\" , text)\n",
    "    #Remove double spaces\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_spacing(text):\n",
    "    #Remove all spaces and replace them with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    #Remove spaces before and after punctuation\n",
    "    text = re.sub(r'\\s+([.,!?])', r'\\1', text)\n",
    "    text = re.sub(r'([.,!?])\\s+', r'\\1', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "#Implement lemmatization, group words by root stem but keep the different tenses \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemm_text(text):\n",
    "    ans = lemmatizer.lemmatize(text)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reviews['text'] = full_reviews['text'].apply(correct_text)\n",
    "full_reviews['text'] = full_reviews['text'].apply(standardize_text)\n",
    "full_reviews['text'] = full_reviews['text'].apply(lemm_text)\n",
    "full_reviews['text'] = full_reviews['text'].apply(remove_spacing)\n",
    "\n",
    "full_reviews = shuffle(full_reviews)\n",
    "\n",
    "full_reviews.to_csv('full_reviews_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reviews = pd.read_csv('full_reviews_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871\n"
     ]
    }
   ],
   "source": [
    "#Prepare dataset for model use \n",
    "#Get the max token from data to be used in model\n",
    "max_length = 0\n",
    "for row in full_reviews['text']:\n",
    "    if len(row.split(\" \")) > max_length:\n",
    "        max_length = len(row.split(\" \"))\n",
    "\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_review_batch = full_reviews[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create datasets of predictors and labels\n",
    "predictors_mini = mini_review_batch['text'].values \n",
    "labels_mini = mini_review_batch['real'].values\n",
    "\n",
    "#Split the data into test / train 70%/30%\n",
    "x_train, x_test, y_train, y_test = train_test_split(predictors_mini, labels_mini, test_size=0.3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500,)\n",
      "(3500,)\n",
      "(1500,)\n",
      "(1500,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "#Reshape the outputs for use in models\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "#Apply the one hot encode\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500,)\n",
      "(3500,)\n",
      "(1500,)\n",
      "(1500,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create Embeddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained Word2Vec\n",
    "import tensorflow_hub as hub\n",
    "embed = hub.load('Embed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_enc(reviews):\n",
    "    #store the encoded reviews\n",
    "    encoded_reviews = []\n",
    "    #Iterate through the list of reviews\n",
    "    for review in reviews:\n",
    "        #split review\n",
    "        tokens = review.split(\" \")\n",
    "        word2vec_embedding = embed(tokens)\n",
    "        encoded_reviews.append(word2vec_embedding)\n",
    "    return encoded_reviews\n",
    "\n",
    "def get_padded_encoded_reviews(encoded_reviews):\n",
    "    #Pad the reviews\n",
    "    padded_reviews_encoding = []\n",
    "    \n",
    "    for enc_review in encoded_reviews:\n",
    "        #Calculate the number of zeros to pad the review with\n",
    "        zero_padding_cnt = max_length - enc_review.shape[0]\n",
    "        pad = np.zeros((1, 500))\n",
    "        #Iterate over the number of zeros to pad\n",
    "        for i in range(zero_padding_cnt):\n",
    "            #Connect the pad array and the encoded review along the 0th axis\n",
    "            enc_review = np.concatenate((pad, enc_review), axis=0)\n",
    "        padded_reviews_encoding.append(enc_review)\n",
    "    return padded_reviews_encoding\n",
    "\n",
    "\n",
    "def label_encode(label):\n",
    "    # Encode the label as a one-hot encoding\n",
    "    encoded_label = np.where(label == 1, [0, 1], [1, 0])\n",
    "    return encoded_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the reviews\n",
    "x_train_padded_encoded = get_padded_encoded_reviews(get_word2vec_enc(x_train))\n",
    "x_test_padded_encoded = get_padded_encoded_reviews(get_word2vec_enc(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the labels\n",
    "encoded_train_label = [label_encode(label) for label in y_train]\n",
    "encoded_test_label = [label_encode(label) for label in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn values into numpy arrays so can be used in model\n",
    "train_data = np.array(x_train_padded_encoded)\n",
    "train_label = np.array(encoded_train_label)\n",
    "\n",
    "test_data = np.array(x_test_padded_encoded)\n",
    "test_label = np.array(encoded_test_label)\n",
    "#Save locally\n",
    "np.save('train_data', train_data)\n",
    "np.save('train_label', train_label)\n",
    "np.save('test_data', test_data)\n",
    "np.save('test_label', test_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Breakpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('train_data.npy')\n",
    "train_label= np.load('train_label.npy')\n",
    "\n",
    "test_data = np.load('test_data.npy')\n",
    "test_label = np.load('test_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_mini = train_data[0:1000]\n",
    "train_label_mini = train_label[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 871, 500)\n",
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_mini.shape)\n",
    "print(train_label_mini.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " ...\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_label_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model's architecture\n",
    "def build_model():\n",
    "  model = tf.keras.models.Sequential()\n",
    "  model.add(tf.keras.layers.Conv1D(128, (9),activation='relu'))\n",
    "  model.add(tf.keras.layers.Dropout(0.3))\n",
    "  model.add(tf.keras.layers.MaxPooling1D((2)))\n",
    "  model.add(tf.keras.layers.Dropout(0.2))\n",
    "  model.add(tf.keras.layers.Conv1D(64, (7),activation='relu'))\n",
    "  model.add(tf.keras.layers.Dropout(0.1))\n",
    "  model.add(tf.keras.layers.MaxPooling1D((2)))\n",
    "  model.add(tf.keras.layers.Dropout(0.1))\n",
    "  model.add(tf.keras.layers.Conv1D(32, (5),activation='relu'))\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(2, activation='sigmoid'))\n",
    "  return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - 12s 161ms/step - loss: 0.4486 - accuracy: 0.7771 - val_loss: 0.2640 - val_accuracy: 0.9200\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 1s 47ms/step - loss: 0.2460 - accuracy: 0.9057 - val_loss: 0.3589 - val_accuracy: 0.8500\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.1985 - accuracy: 0.9114 - val_loss: 0.1990 - val_accuracy: 0.9167\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.1124 - accuracy: 0.9557 - val_loss: 0.1020 - val_accuracy: 0.9700\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.0712 - accuracy: 0.9757 - val_loss: 0.1292 - val_accuracy: 0.9433\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.0330 - accuracy: 0.9857 - val_loss: 0.1317 - val_accuracy: 0.9367\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.0269 - accuracy: 0.9914 - val_loss: 0.1796 - val_accuracy: 0.9300\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0381 - accuracy: 0.9857 - val_loss: 0.1525 - val_accuracy: 0.9333\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.0202 - accuracy: 0.9914 - val_loss: 0.0346 - val_accuracy: 0.9867\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0341 - accuracy: 0.9900 - val_loss: 0.0674 - val_accuracy: 0.9733\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_data_mini, \n",
    "    train_label_mini, \n",
    "    epochs = 10,\n",
    "    batch_size = 32,\n",
    "    validation_split=0.3,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 - 3s - loss: 0.0859 - accuracy: 0.9700 - 3s/epoch - 69ms/step\n",
      "Test score: 0.08588975667953491\n",
      "Test accuracy: 0.9700000286102295\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(test_data, test_label, verbose=2)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[0.02165707 0.9794042 ]]\n",
      "[[0 1]]\n",
      "*******\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[0.7820984  0.20605727]]\n",
      "[[1 0]]\n",
      "*******\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[0.9691758  0.11756897]]\n",
      "[[1 0]]\n",
      "*******\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[0.7714792  0.49747628]]\n",
      "[[1 0]]\n",
      "*******\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[0.06119137 0.96922696]]\n",
      "[[0 1]]\n",
      "*******\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[0.03065663 0.98833585]]\n",
      "[[0 1]]\n",
      "*******\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[0.02175694 0.98077285]]\n",
      "[[0 1]]\n",
      "*******\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[0.02154815 0.97997427]]\n",
      "[[0 1]]\n",
      "*******\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[[0.01725633 0.99079764]]\n",
      "[[0 1]]\n",
      "*******\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "[[0.02771874 0.9763532 ]]\n",
      "[[0 1]]\n",
      "*******\n"
     ]
    }
   ],
   "source": [
    "for i in range (10):\n",
    "    x = 456+i\n",
    "    print(model.predict(test_data[x:x+1]))\n",
    "    print(test_label[x:x+1])\n",
    "    print('*******')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
