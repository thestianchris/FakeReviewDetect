{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**\n",
    "\n",
    "Now that the fake reviews have been generated using the re-trained gpt2 model, its time to prepare the datasets for final use within the classification model; this is done through standardizing the data and then tokenisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = '/tmp/tfhub'\n",
    "\n",
    "#Load datasets\n",
    "fake_reviews = pd.read_csv('reviews_generated.csv',usecols=['text'])\n",
    "real_reviews = pd.read_csv('bigreviews.csv',usecols=['text'])\n",
    "\n",
    "#Add new column indicating real or fake\n",
    "#real = 1 / fake = 0\n",
    "real_reviews['real'] = 1\n",
    "fake_reviews['real'] = 0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Rate of 33% of reviews suspected to be fake, so dataset will be made with this concept in mind*\n",
    "\n",
    "Generated = 7252 \n",
    "\n",
    "Real = 14508\n",
    "\n",
    "Total = 21756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the last 21756 reviews\n",
    "real_reviews = real_reviews.tail(14504)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join databases\n",
    "full_reviews = pd.concat([real_reviews, fake_reviews], ignore_index=True)\n",
    "full_reviews.to_csv('full_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  real\n",
      "0      My MacBook Pro retina was failing do the stupi...     1\n",
      "1      My boyfriend and I found this place doing a lo...     1\n",
      "2      Hubby and I decided to try.  Never been to Ger...     1\n",
      "3      Ok so this is really Aneu! I really don't know...     1\n",
      "4      Finally got to try Smee's recently.  I like th...     1\n",
      "...                                                  ...   ...\n",
      "21751  we were looking for a place to eat and we foun...     0\n",
      "21752  second time here.  the food is good, but the s...     0\n",
      "21753  tucked on 76, it's a great place to go to for ...     0\n",
      "21754  these hand grenades are the best! \\n\\nthe staf...     0\n",
      "21755  this is totallly a great place to go for a cas...     0\n",
      "\n",
      "[21756 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(full_reviews)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Clean the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reviews = pd.read_csv('full_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization and spell check\n",
    "import itertools\n",
    "import re\n",
    "from autocorrect import Speller\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def correct_text(text):\n",
    "    #One letter in a word should not be present more than twice in continuation\n",
    "    text_correction = ''.join(''.join(s)[:3] for _, s in itertools.groupby(text))\n",
    "    #Apply autocorrection to the corrected text\n",
    "    spell = Speller(lang='en')\n",
    "    ans = spell(text_correction)\n",
    "    return ans\n",
    "\n",
    "\n",
    "def standardize_text(text):\n",
    "    #Remove unicode characters\n",
    "    text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "    #Turn to lower case\n",
    "    text = text.lower()\n",
    "    #Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    #Remove punctuation\n",
    "    text = re.sub(\"[^-9A-Za-z ]\", \"\" , text)\n",
    "    #Remove double spaces\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_spacing(text):\n",
    "    #Remove all spaces and replace them with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    #Remove spaces before and after punctuation\n",
    "    text = re.sub(r'\\s+([.,!?])', r'\\1', text)\n",
    "    text = re.sub(r'([.,!?])\\s+', r'\\1', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "#Implement lemmatization, group words by root stem but keep the different tenses \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemm_text(text):\n",
    "    ans = lemmatizer.lemmatize(text)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reviews['text'] = full_reviews['text'].apply(correct_text)\n",
    "full_reviews['text'] = full_reviews['text'].apply(standardize_text)\n",
    "full_reviews['text'] = full_reviews['text'].apply(lemm_text)\n",
    "full_reviews['text'] = full_reviews['text'].apply(remove_spacing)\n",
    "\n",
    "full_reviews = shuffle(full_reviews)\n",
    "\n",
    "full_reviews.to_csv('full_reviews_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reviews = pd.read_csv('full_reviews_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871\n"
     ]
    }
   ],
   "source": [
    "#Prepare dataset for model use \n",
    "#Get the max token from data to be used in model\n",
    "max_length = 0\n",
    "for row in full_reviews['text']:\n",
    "    if len(row.split(\" \")) > max_length:\n",
    "        max_length = len(row.split(\" \"))\n",
    "\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_review_batch = full_reviews[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create datasets of predictors and labels\n",
    "predictors_mini = mini_review_batch['text'].values \n",
    "labels_mini = mini_review_batch['real'].values\n",
    "\n",
    "#Split the data into test / train 70%/30%\n",
    "x_train, x_test, y_train, y_test = train_test_split(predictors_mini, labels_mini, test_size=0.3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500,)\n",
      "(3500,)\n",
      "(1500,)\n",
      "(1500,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "#Reshape the outputs for use in models\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "#Apply the one hot encode\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500,)\n",
      "(3500,)\n",
      "(1500,)\n",
      "(1500,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create Embeddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained Word2Vec\n",
    "import tensorflow_hub as hub\n",
    "embed = hub.load('Embed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_enc(reviews):\n",
    "    #store the encoded reviews\n",
    "    encoded_reviews = []\n",
    "    #Iterate through the list of reviews\n",
    "    for review in reviews:\n",
    "        #split review\n",
    "        tokens = review.split(\" \")\n",
    "        word2vec_embedding = embed(tokens)\n",
    "        encoded_reviews.append(word2vec_embedding)\n",
    "    return encoded_reviews\n",
    "\n",
    "def get_padded_encoded_reviews(encoded_reviews):\n",
    "    #Pad the reviews\n",
    "    padded_reviews_encoding = []\n",
    "    \n",
    "    for enc_review in encoded_reviews:\n",
    "        #Calculate the number of zeros to pad the review with\n",
    "        zero_padding_cnt = max_length - enc_review.shape[0]\n",
    "        pad = np.zeros((1, 500))\n",
    "        #Iterate over the number of zeros to pad\n",
    "        for i in range(zero_padding_cnt):\n",
    "            #Connect the pad array and the encoded review along the 0th axis\n",
    "            enc_review = np.concatenate((pad, enc_review), axis=0)\n",
    "        padded_reviews_encoding.append(enc_review)\n",
    "    return padded_reviews_encoding\n",
    "\n",
    "\n",
    "def label_encode(label):\n",
    "    # Encode the label as a one-hot encoding\n",
    "    encoded_label = np.where(label == 1, [0, 1], [1, 0])\n",
    "    return encoded_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the reviews\n",
    "x_train_padded_encoded = get_padded_encoded_reviews(get_word2vec_enc(x_train))\n",
    "x_test_padded_encoded = get_padded_encoded_reviews(get_word2vec_enc(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the labels\n",
    "encoded_train_label = [label_encode(label) for label in y_train]\n",
    "encoded_test_label = [label_encode(label) for label in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn values into numpy arrays so can be used in model\n",
    "train_data = np.array(x_train_padded_encoded)\n",
    "train_label = np.array(encoded_train_label)\n",
    "\n",
    "test_data = np.array(x_test_padded_encoded)\n",
    "test_label = np.array(encoded_test_label)\n",
    "#Save locally\n",
    "np.save('train_data', train_data)\n",
    "np.save('train_label', train_label)\n",
    "np.save('test_data', test_data)\n",
    "np.save('test_label', test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_mini = train_data[0:1000]\n",
    "train_label_mini = train_label[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 871, 500)\n",
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_mini.shape)\n",
    "print(train_label_mini.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model's architecture\n",
    "def build_model():\n",
    "  model = tf.keras.models.Sequential()\n",
    "  model.add(tf.keras.layers.Conv1D(128, (9),activation='relu'))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.MaxPooling1D((2)))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Conv1D(128, (7),activation='relu'))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.MaxPooling1D((2)))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "  model.add(tf.keras.layers.Conv1D(128, (5),activation='relu'))\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(2, activation='sigmoid'))\n",
    "  return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mz:\\Uni Work\\Final Project 3\\Final Project- Live\\FakeReviewDetect\\Final-Classify.ipynb Cell 28\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/Uni%20Work/Final%20Project%203/Final%20Project-%20Live/FakeReviewDetect/Final-Classify.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_data \u001b[39m=\u001b[39m train_data\u001b[39m.\u001b[39mreshape(train_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m871\u001b[39m, \u001b[39m500\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/z%3A/Uni%20Work/Final%20Project%203/Final%20Project-%20Live/FakeReviewDetect/Final-Classify.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/Uni%20Work/Final%20Project%203/Final%20Project-%20Live/FakeReviewDetect/Final-Classify.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_data, \n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/Uni%20Work/Final%20Project%203/Final%20Project-%20Live/FakeReviewDetect/Final-Classify.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train_label, \n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/Uni%20Work/Final%20Project%203/Final%20Project-%20Live/FakeReviewDetect/Final-Classify.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     epochs \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/Uni%20Work/Final%20Project%203/Final%20Project-%20Live/FakeReviewDetect/Final-Classify.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     batch_size \u001b[39m=\u001b[39;49m \u001b[39m32\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/Uni%20Work/Final%20Project%203/Final%20Project-%20Live/FakeReviewDetect/Final-Classify.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/Uni%20Work/Final%20Project%203/Final%20Project-%20Live/FakeReviewDetect/Final-Classify.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m ) \n",
      "File \u001b[1;32mc:\\Users\\Christian Gauthier\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Christian Gauthier\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "train_data = train_data.reshape(train_data.shape[0], 871, 500)\n",
    "history = model.fit(\n",
    "    train_data, \n",
    "    train_label, \n",
    "    epochs = 10,\n",
    "    batch_size = 32,\n",
    "    validation_split=0.3,\n",
    ") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
