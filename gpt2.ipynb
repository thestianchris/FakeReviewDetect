{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christian Gauthier\\miniconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#Import dependancies\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2') #Encoder text to tensor\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian Gauthier\\AppData\\Local\\Temp\\ipykernel_22156\\3893656625.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  reviews['text'].str.replace('[^\\w\\s]','') #Remove punctuation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 1006\n",
      "57104    this was one of my boyfriend's favorite places...\n",
      "65110    birchtree catered and pretty much helped plan ...\n",
      "32763    ray's is my favorite place to get subs in nj. ...\n",
      "36231    i booked an appointment this afternoon for bot...\n",
      "85442    the restaurant is small so it produced a long ...\n",
      "                               ...                        \n",
      "27292    loved this place. after an unpleasant experien...\n",
      "80798    on top of it like piraat ale. great grub. bone...\n",
      "63060    so a friend and i went to this pakistani \"hole...\n",
      "88494    good pad thai and spring rolls. fast service e...\n",
      "50632    so sick!!!  staff was terrible and the produce...\n",
      "Name: text, Length: 5000, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian Gauthier\\AppData\\Local\\Temp\\ipykernel_22156\\3893656625.py:22: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  reviews = reviews[:5000]\n"
     ]
    }
   ],
   "source": [
    "#Get Yelp dataset and standardise\n",
    "\n",
    "reviews = pd.read_csv('reviews.csv')\n",
    "reviews = shuffle(reviews)\n",
    "\n",
    "#Standardise and tokenize\n",
    "for column in reviews:\n",
    "    reviews['text'] = reviews['text'].str.lower()   #Covert the text to lower case\n",
    "    reviews['text'].str.replace('[^\\w\\s]','') #Remove punctuation\n",
    "    reviews['text'].str.strip() #Remove whitespace\n",
    "    reviews['text'].str.replace(\"\\n\", \" \") #Remove escape characters\n",
    "reviews = reviews['text']\n",
    "\n",
    "#Get the max token from training data to be used in model\n",
    "max_length = 0\n",
    "for row in reviews:\n",
    "    if len(row.split(\" \")) > max_length:\n",
    "        max_length = len(row.split(\" \"))\n",
    "\n",
    "print('max length: ' + str(max_length))\n",
    "\n",
    "reviews = reviews[:5000]\n",
    "\n",
    "print(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(reviews.iloc[1], return_tensors='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (653274 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "single_string = ''\n",
    "for row in reviews:\n",
    "  x = row\n",
    "  single_string += x \n",
    "string_tokenized = tokenizer.encode(single_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "block_size = 100\n",
    "BATCH_SIZE = 12\n",
    "BUFFER_SIZE = 1000\n",
    "for i in range(0, len(string_tokenized) - block_size + 1, block_size):\n",
    "  examples.append(string_tokenized[i:i + block_size])\n",
    "inputs, labels = [], []\n",
    "for ex in examples:\n",
    "  inputs.append(ex[:-1])\n",
    "  labels.append(ex[1:])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "# definining loss function\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# defining our metric which we want to observe\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# compiling the model\n",
    "model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "544/544 [==============================] - 136s 228ms/step - loss: 3.7692 - accuracy: 0.3066\n",
      "Epoch 2/25\n",
      "544/544 [==============================] - 124s 229ms/step - loss: 3.5566 - accuracy: 0.3270\n",
      "Epoch 3/25\n",
      "544/544 [==============================] - 125s 229ms/step - loss: 3.4539 - accuracy: 0.3374\n",
      "Epoch 4/25\n",
      "544/544 [==============================] - 125s 230ms/step - loss: 3.3672 - accuracy: 0.3472\n",
      "Epoch 5/25\n",
      "544/544 [==============================] - 125s 229ms/step - loss: 3.2917 - accuracy: 0.3551\n",
      "Epoch 6/25\n",
      "544/544 [==============================] - 125s 229ms/step - loss: 3.2212 - accuracy: 0.3629\n",
      "Epoch 7/25\n",
      "544/544 [==============================] - 125s 229ms/step - loss: 3.1513 - accuracy: 0.3713\n",
      "Epoch 8/25\n",
      "544/544 [==============================] - 121s 222ms/step - loss: 3.0844 - accuracy: 0.3793\n",
      "Epoch 9/25\n",
      "544/544 [==============================] - 119s 219ms/step - loss: 3.0197 - accuracy: 0.3867\n",
      "Epoch 10/25\n",
      "544/544 [==============================] - 119s 219ms/step - loss: 2.9551 - accuracy: 0.3945\n",
      "Epoch 11/25\n",
      "544/544 [==============================] - 119s 218ms/step - loss: 2.8933 - accuracy: 0.4033\n",
      "Epoch 12/25\n",
      "544/544 [==============================] - 119s 219ms/step - loss: 2.8318 - accuracy: 0.4107\n",
      "Epoch 13/25\n",
      "544/544 [==============================] - 119s 219ms/step - loss: 2.7733 - accuracy: 0.4183\n",
      "Epoch 14/25\n",
      "544/544 [==============================] - 119s 219ms/step - loss: 2.7134 - accuracy: 0.4265\n",
      "Epoch 15/25\n",
      "544/544 [==============================] - 119s 219ms/step - loss: 2.6569 - accuracy: 0.4341\n",
      "Epoch 16/25\n",
      "544/544 [==============================] - 119s 219ms/step - loss: 2.5988 - accuracy: 0.4426\n",
      "Epoch 17/25\n",
      "544/544 [==============================] - 119s 219ms/step - loss: 2.5441 - accuracy: 0.4502\n",
      "Epoch 18/25\n",
      "544/544 [==============================] - 119s 219ms/step - loss: 2.4895 - accuracy: 0.4583\n",
      "Epoch 19/25\n",
      "544/544 [==============================] - 119s 219ms/step - loss: 2.4348 - accuracy: 0.4664\n",
      "Epoch 20/25\n",
      "544/544 [==============================] - 119s 219ms/step - loss: 2.3806 - accuracy: 0.4744\n",
      "Epoch 21/25\n",
      "544/544 [==============================] - 119s 219ms/step - loss: 2.3278 - accuracy: 0.4831\n",
      "Epoch 22/25\n",
      "544/544 [==============================] - 119s 219ms/step - loss: 2.2771 - accuracy: 0.4908\n",
      "Epoch 23/25\n",
      "544/544 [==============================] - 119s 219ms/step - loss: 2.2274 - accuracy: 0.4987\n",
      "Epoch 24/25\n",
      "544/544 [==============================] - 119s 219ms/step - loss: 2.1748 - accuracy: 0.5074\n",
      "Epoch 25/25\n",
      "544/544 [==============================] - 119s 219ms/step - loss: 2.1272 - accuracy: 0.5149\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For dinner, my wife and i split an appetizer and two entrees. we split the crab cakes and macaroni and cheese. both were very good. they were fresh and had a nice balance of flavor. the macarons were also very tasty. i would definitely come back for more. \n",
      "the service was great. our waiter was very attentive and took the time to check on us and make sure we were enjoying the food we ordered. it was a very pleasant experience and we look forward to trying everything else on their menu!i love this place! i've been going here for years and it's one of my favorite places in town! the staff is always friendly and the prices are very reasonable! they have a wide\n"
     ]
    }
   ],
   "source": [
    "text = \"For dinner,\"\n",
    "# encoding the input text\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "# getting out output\n",
    "output = model.generate(\n",
    "  input_ids,\n",
    "  max_length = 150,\n",
    "  num_beams = 5,\n",
    "  temperature = 0.8,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=5\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best part was the service. we were seated quickly and the place was clean. the staff was friendly and attentive. i would highly recommend this restaurant.i love this place! i've been coming here for years and it's always a good experience. they have a great variety of sandwiches, pastas, and pastries. my favorite is the sweet potato pastrami and my all time favorite the chicken and waffles. \n",
      "the only downside i can think of is that they don't always have happy hour so make sure you go during the week in order to save a few bucks. but i will definitely come back for sure!i'm a big fan of la colombe burrito and this is by far the best bur\n"
     ]
    }
   ],
   "source": [
    "text = \"The best part was\"\n",
    "# encoding the input text\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "# getting out output\n",
    "output = model.generate(\n",
    "  input_ids,\n",
    "  max_length = 150,\n",
    "  num_beams = 5,\n",
    "  temperature = 0.85,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=5\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every time we go, we are greeted with a warm smile and a knowing nod from our waiter.  it is a great way to give back to our community and show that we care about our food and are not afraid to try something new.i love this place. i've been going here for years and i can't say enough good things about the staff. they are always friendly and the food is always great. the only downside is the price. it's a little expensive for what you get\n"
     ]
    }
   ],
   "source": [
    "text = \"Every time we go,\"\n",
    "# encoding the input text\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "# getting out output\n",
    "output = model.generate(\n",
    "  input_ids,\n",
    "  max_length = 100,\n",
    "  num_beams = 5,\n",
    "  temperature = 0.85,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=5\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Their menu changes all the time, so it's hard to know what to expect.  i'm not sure if we'll be back next year, but we will definitely stop in for the oysters and the crawfish bisque.my husband and i have been going to this place since we were little. we love it. the staff is always friendly and helpful. they have a wide variety of specialty pizzas to choose from. i always get the calabacitas and they are delicious. it is a little pricey for what you get but the portions are very generous. if you are looking for something different then this\n"
     ]
    }
   ],
   "source": [
    "text = \"Their menu changes\"\n",
    "# encoding the input text\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "# getting out output\n",
    "output = model.generate(\n",
    "  input_ids,\n",
    "  max_length = 125,\n",
    "  num_beams = 5,\n",
    "  temperature = 0.92,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=5\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our waitress was very nice and attentive.  i ordered the shrimp and grits and it was good but the sauce was too sweet for my taste. i will definitely go back to try other dishes on the menu.this is a great place to get a quick bite to eat while in the tampa bay area. they have a wide variety of sandwiches, salads, and smoothies to choose from. the staff is always friendly and always willing to help you get the most out of your visit.my husband and i have been going to this place since we were little. we love it. it's tucked away in a shopping center right by the airport. you can walk in and get free chips and salsa to dip into the delicious salsa bar. service is quick and friendly. food is fresh and delicious. my husband loves his burritos and queso burrito. their tortillas are huge and are made to order. this is one of our favorite places to stop for lunch or dinner.\n"
     ]
    }
   ],
   "source": [
    "text = \"Our waitress\"\n",
    "# encoding the input text\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "# getting out output\n",
    "output = model.generate(\n",
    "  input_ids,\n",
    "  max_length = 200,\n",
    "  num_beams = 5,\n",
    "  temperature = 0.845,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=5\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When I walked in, it was already dark and dingy with only a few tables in the place.  i walked out and asked the host if there was anything else they could do to make my meal better, and he said he would look into it, but that he couldn't do anything since they were closing in on a saturday night.\n",
      "i ordered the chicken and waffles, which were good but nothing to write home about. the waffle was so dense and dense that i had to eat it with a fork and knife. when i got home and looked for the kitchen where the food was being prepared, i realized there wasn't one in there. instead, there were two dirty silverware stacked on the counter. one was dripping wet with grease, one of which had been sitting in my food. i could only see the grease dripping down the side of the panini bread with my fork. it looked like it had come from the sink or some other dirty sink fixture. my order of fries had grease all over them, including the ones with the lollipop in them. they tasted like they came from a pre-mixed can of tomato sauce that was sitting on top of a greasy knife wound around the plate\n"
     ]
    }
   ],
   "source": [
    "text = \"When I walked in,\"\n",
    "# encoding the input text\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "# getting out output\n",
    "output = model.generate(\n",
    "  input_ids,\n",
    "  max_length = 250,\n",
    "  num_beams = 5,\n",
    "  temperature = 0.82,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=5\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9396d7d05074184ad1bdffa89c9baf4b70aedb99cfe5c454adbba107973e00ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
