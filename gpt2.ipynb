{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christian Gauthier\\miniconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#Import dependancies\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2') #Encoder text to tensor\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id) #load the model, using the smallest model for this prototype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian Gauthier\\AppData\\Local\\Temp\\ipykernel_6380\\4107130471.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  reviews['text'].str.replace('[^\\w\\s]','') #Remove punctuation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47739    excellent property if you are in the oldsmar, ...\n",
      "44739    so glad we found this place. it has everything...\n",
      "14980    so i've tried this place a couple times before...\n",
      "71427    luke's is awesome! if your looking for some pl...\n",
      "66821    i love the grilled chicken parmesan salad at t...\n",
      "                               ...                        \n",
      "72868    i recently moved here from santa monica, where...\n",
      "4104     really, more like 3.5 stars. very clean, light...\n",
      "9021     there's honestly nothing wrong with famous dav...\n",
      "3112     i can't wait to go back. it's unbelievably ful...\n",
      "60970    our first stay wound up to be a debacle, mainl...\n",
      "Name: text, Length: 25000, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian Gauthier\\AppData\\Local\\Temp\\ipykernel_6380\\4107130471.py:15: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  reviews = reviews[:25000]\n"
     ]
    }
   ],
   "source": [
    "#Get Yelp dataset and standardise\n",
    "\n",
    "reviews = pd.read_csv('reviews.csv')\n",
    "reviews = shuffle(reviews)\n",
    "\n",
    "#Standardise and tokenize\n",
    "for column in reviews:\n",
    "    reviews['text'] = reviews['text'].str.lower()   #Covert the text to lower case\n",
    "    reviews['text'].str.replace('[^\\w\\s]','') #Remove punctuation\n",
    "    reviews['text'].str.strip() #Remove whitespace\n",
    "    reviews['text'].str.replace(\"\\n\", \" \") #Remove escape characters\n",
    "reviews = reviews['text']\n",
    "\n",
    "#Use only a slice of the data 25k reviews\n",
    "reviews = reviews[:25000]\n",
    "\n",
    "print(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3230940 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#turn the entire 25000 reviews into a long string to be segmented in next sequence\n",
    "single_string = ''\n",
    "for row in reviews:\n",
    "  x = row\n",
    "  single_string += x \n",
    "string_tokenized = tokenizer.encode(single_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables to store the data\n",
    "examples = []\n",
    "block_size = 100\n",
    "BATCH_SIZE = 12\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "# Split the string_tokenized list into blocks of size block_size\n",
    "# and store each block in the examples list\n",
    "for i in range(0, len(string_tokenized) - block_size + 1, block_size):\n",
    "  examples.append(string_tokenized[i:i + block_size])\n",
    "\n",
    "# Initialize empty lists for inputs and labels\n",
    "inputs, labels = [], []\n",
    "\n",
    "# For each example in the examples list,\n",
    "# store the input as the example without the last element\n",
    "# and store the label as the example without the first element\n",
    "for ex in examples:\n",
    "  inputs.append(ex[:-1])\n",
    "  labels.append(ex[1:])\n",
    "\n",
    "# Create a dataset from the inputs and labels tensors\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "\n",
    "# Shuffle the dataset and batch it\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining optimizer\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "# definining loss function\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# defining our metric which we want to observe\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# compiling the model\n",
    "model.compile(optimizer='adam', loss=[loss, *[None] * model.config.n_layer], metrics=[metric])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running model with 25000 reviews, 100 block size, takes around 8:30 min per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2692/2692 [==============================] - 526s 192ms/step - loss: 3.8531 - accuracy: 0.2918\n",
      "Epoch 2/20\n",
      "2692/2692 [==============================] - 519s 193ms/step - loss: 3.5107 - accuracy: 0.3190\n",
      "Epoch 3/20\n",
      "2692/2692 [==============================] - 510s 190ms/step - loss: 3.2538 - accuracy: 0.3419\n",
      "Epoch 4/20\n",
      "2692/2692 [==============================] - 492s 183ms/step - loss: 3.0075 - accuracy: 0.3674\n",
      "Epoch 5/20\n",
      "2692/2692 [==============================] - 493s 183ms/step - loss: 2.7701 - accuracy: 0.3962\n",
      "Epoch 6/20\n",
      "2692/2692 [==============================] - 510s 190ms/step - loss: 2.5472 - accuracy: 0.4259\n",
      "Epoch 7/20\n",
      "2692/2692 [==============================] - 541s 201ms/step - loss: 2.3427 - accuracy: 0.4557\n",
      "Epoch 8/20\n",
      "2692/2692 [==============================] - 564s 209ms/step - loss: 2.1560 - accuracy: 0.4851\n",
      "Epoch 9/20\n",
      "2692/2692 [==============================] - 571s 212ms/step - loss: 1.9868 - accuracy: 0.5132\n",
      "Epoch 10/20\n",
      "2692/2692 [==============================] - 526s 195ms/step - loss: 1.8337 - accuracy: 0.5398\n",
      "Epoch 11/20\n",
      "2692/2692 [==============================] - 531s 197ms/step - loss: 1.6946 - accuracy: 0.5653\n",
      "Epoch 12/20\n",
      "2692/2692 [==============================] - 531s 197ms/step - loss: 1.5677 - accuracy: 0.5902\n",
      "Epoch 13/20\n",
      "2692/2692 [==============================] - 528s 196ms/step - loss: 1.4528 - accuracy: 0.6130\n",
      "Epoch 14/20\n",
      "2692/2692 [==============================] - 528s 196ms/step - loss: 1.3500 - accuracy: 0.6344\n",
      "Epoch 15/20\n",
      "2692/2692 [==============================] - 524s 195ms/step - loss: 1.2555 - accuracy: 0.6550\n",
      "Epoch 16/20\n",
      "1432/2692 [==============>...............] - ETA: 4:19 - loss: 1.1808 - accuracy: 0.6714"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For dinner, my wife and i split an appetizer and two entrees. we split the crab cakes and macaroni and cheese. both were very good. they were fresh and had a nice balance of flavor. the macarons were also very tasty. i would definitely come back for more. \n",
      "the service was great. our waiter was very attentive and took the time to check on us and make sure we were enjoying the food we ordered. it was a very pleasant experience and we look forward to trying everything else on their menu!i love this place! i've been going here for years and it's one of my favorite places in town! the staff is always friendly and the prices are very reasonable! they have a wide\n"
     ]
    }
   ],
   "source": [
    "text = \"For what it is\"\n",
    "# encoding the input text\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "# getting out output\n",
    "output = model.generate(\n",
    "  input_ids,\n",
    "  max_length = 150,\n",
    "  num_beams = 5,\n",
    "  temperature = 0.8,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=5\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best part was the service. we were seated quickly and the place was clean. the staff was friendly and attentive. i would highly recommend this restaurant.i love this place! i've been coming here for years and it's always a good experience. they have a great variety of sandwiches, pastas, and pastries. my favorite is the sweet potato pastrami and my all time favorite the chicken and waffles. \n",
      "the only downside i can think of is that they don't always have happy hour so make sure you go during the week in order to save a few bucks. but i will definitely come back for sure!i'm a big fan of la colombe burrito and this is by far the best bur\n"
     ]
    }
   ],
   "source": [
    "text = \"The restaurant\"\n",
    "# encoding the input text\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "# getting out output\n",
    "output = model.generate(\n",
    "  input_ids,\n",
    "  max_length = 150,\n",
    "  num_beams = 5,\n",
    "  temperature = 0.85,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=5\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every time we go, we are greeted with a warm smile and a knowing nod from our waiter.  it is a great way to give back to our community and show that we care about our food and are not afraid to try something new.i love this place. i've been going here for years and i can't say enough good things about the staff. they are always friendly and the food is always great. the only downside is the price. it's a little expensive for what you get\n"
     ]
    }
   ],
   "source": [
    "text = \"I think they're\"\n",
    "# encoding the input text\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "# getting out output\n",
    "output = model.generate(\n",
    "  input_ids,\n",
    "  max_length = 100,\n",
    "  num_beams = 5,\n",
    "  temperature = 0.85,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=5\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Their menu changes all the time, so it's hard to know what to expect.  i'm not sure if we'll be back next year, but we will definitely stop in for the oysters and the crawfish bisque.my husband and i have been going to this place since we were little. we love it. the staff is always friendly and helpful. they have a wide variety of specialty pizzas to choose from. i always get the calabacitas and they are delicious. it is a little pricey for what you get but the portions are very generous. if you are looking for something different then this\n"
     ]
    }
   ],
   "source": [
    "text = \"I give up on this place\"\n",
    "# encoding the input text\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "# getting out output\n",
    "output = model.generate(\n",
    "  input_ids,\n",
    "  max_length = 125,\n",
    "  num_beams = 5,\n",
    "  temperature = 0.92,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=5\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Unfortunately,\"\n",
    "# encoding the input text\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "# getting out output\n",
    "output = model.generate(\n",
    "  input_ids,\n",
    "  max_length = 150,\n",
    "  num_beams = 5,\n",
    "  temperature = 0.8,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=5\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9396d7d05074184ad1bdffa89c9baf4b70aedb99cfe5c454adbba107973e00ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
