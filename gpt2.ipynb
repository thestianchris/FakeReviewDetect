{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christian Gauthier\\miniconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#Import dependancies\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2') #Encoder text to tensor\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id) #load the model, using the smallest model for this prototype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian Gauthier\\AppData\\Local\\Temp\\ipykernel_6380\\4107130471.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  reviews['text'].str.replace('[^\\w\\s]','') #Remove punctuation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47739    excellent property if you are in the oldsmar, ...\n",
      "44739    so glad we found this place. it has everything...\n",
      "14980    so i've tried this place a couple times before...\n",
      "71427    luke's is awesome! if your looking for some pl...\n",
      "66821    i love the grilled chicken parmesan salad at t...\n",
      "                               ...                        \n",
      "72868    i recently moved here from santa monica, where...\n",
      "4104     really, more like 3.5 stars. very clean, light...\n",
      "9021     there's honestly nothing wrong with famous dav...\n",
      "3112     i can't wait to go back. it's unbelievably ful...\n",
      "60970    our first stay wound up to be a debacle, mainl...\n",
      "Name: text, Length: 25000, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian Gauthier\\AppData\\Local\\Temp\\ipykernel_6380\\4107130471.py:15: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  reviews = reviews[:25000]\n"
     ]
    }
   ],
   "source": [
    "#Get Yelp dataset and standardise\n",
    "\n",
    "reviews = pd.read_csv('reviews.csv')\n",
    "reviews = shuffle(reviews)\n",
    "\n",
    "#Standardise and tokenize\n",
    "for column in reviews:\n",
    "    reviews['text'] = reviews['text'].str.lower()   #Covert the text to lower case\n",
    "    reviews['text'].str.replace('[^\\w\\s]','') #Remove punctuation\n",
    "    reviews['text'].str.strip() #Remove whitespace\n",
    "    reviews['text'].str.replace(\"\\n\", \" \") #Remove escape characters\n",
    "reviews = reviews['text']\n",
    "\n",
    "#Use only a slice of the data 25k reviews\n",
    "reviews = reviews[:25000]\n",
    "\n",
    "print(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3230940 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#turn the entire 25000 reviews into a long string to be segmented in next sequence\n",
    "single_string = ''\n",
    "for row in reviews:\n",
    "  x = row\n",
    "  single_string += x \n",
    "string_tokenized = tokenizer.encode(single_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables to store the data\n",
    "examples = []\n",
    "block_size = 100\n",
    "BATCH_SIZE = 12\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "# Split the string_tokenized list into blocks of size block_size\n",
    "# and store each block in the examples list\n",
    "for i in range(0, len(string_tokenized) - block_size + 1, block_size):\n",
    "  examples.append(string_tokenized[i:i + block_size])\n",
    "\n",
    "# Initialize empty lists for inputs and labels\n",
    "inputs, labels = [], []\n",
    "\n",
    "# For each example in the examples list,\n",
    "# store the input as the example without the last element\n",
    "# and store the label as the example without the first element\n",
    "for ex in examples:\n",
    "  inputs.append(ex[:-1])\n",
    "  labels.append(ex[1:])\n",
    "\n",
    "# Create a dataset from the inputs and labels tensors\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "\n",
    "# Shuffle the dataset and batch it\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining optimizer\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "# definining loss function\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# defining our metric which we want to observe\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# compiling the model\n",
    "model.compile(optimizer='adam', loss=[loss], metrics=[metric])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running model with 25000 reviews, 100 block size, takes around 8:30 min per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2692/2692 [==============================] - 526s 192ms/step - loss: 3.8531 - accuracy: 0.2918\n",
      "Epoch 2/20\n",
      "2692/2692 [==============================] - 519s 193ms/step - loss: 3.5107 - accuracy: 0.3190\n",
      "Epoch 3/20\n",
      "2692/2692 [==============================] - 510s 190ms/step - loss: 3.2538 - accuracy: 0.3419\n",
      "Epoch 4/20\n",
      "2692/2692 [==============================] - 492s 183ms/step - loss: 3.0075 - accuracy: 0.3674\n",
      "Epoch 5/20\n",
      "2692/2692 [==============================] - 493s 183ms/step - loss: 2.7701 - accuracy: 0.3962\n",
      "Epoch 6/20\n",
      "2692/2692 [==============================] - 510s 190ms/step - loss: 2.5472 - accuracy: 0.4259\n",
      "Epoch 7/20\n",
      "2692/2692 [==============================] - 541s 201ms/step - loss: 2.3427 - accuracy: 0.4557\n",
      "Epoch 8/20\n",
      "2692/2692 [==============================] - 564s 209ms/step - loss: 2.1560 - accuracy: 0.4851\n",
      "Epoch 9/20\n",
      "2692/2692 [==============================] - 571s 212ms/step - loss: 1.9868 - accuracy: 0.5132\n",
      "Epoch 10/20\n",
      "2692/2692 [==============================] - 526s 195ms/step - loss: 1.8337 - accuracy: 0.5398\n",
      "Epoch 11/20\n",
      "2692/2692 [==============================] - 531s 197ms/step - loss: 1.6946 - accuracy: 0.5653\n",
      "Epoch 12/20\n",
      "2692/2692 [==============================] - 531s 197ms/step - loss: 1.5677 - accuracy: 0.5902\n",
      "Epoch 13/20\n",
      "2692/2692 [==============================] - 528s 196ms/step - loss: 1.4528 - accuracy: 0.6130\n",
      "Epoch 14/20\n",
      "2692/2692 [==============================] - 528s 196ms/step - loss: 1.3500 - accuracy: 0.6344\n",
      "Epoch 15/20\n",
      "2692/2692 [==============================] - 524s 195ms/step - loss: 1.2555 - accuracy: 0.6550\n",
      "Epoch 16/20\n",
      "2692/2692 [==============================] - 543s 202ms/step - loss: 1.1698 - accuracy: 0.6739\n",
      "Epoch 17/20\n",
      "2692/2692 [==============================] - 528s 196ms/step - loss: 1.0925 - accuracy: 0.6918\n",
      "Epoch 18/20\n",
      "2692/2692 [==============================] - 508s 189ms/step - loss: 1.0205 - accuracy: 0.7085\n",
      "Epoch 19/20\n",
      "2692/2692 [==============================] - 499s 185ms/step - loss: 0.9573 - accuracy: 0.7238\n",
      "Epoch 20/20\n",
      "2692/2692 [==============================] - 499s 185ms/step - loss: 0.8996 - accuracy: 0.7383\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as wte_layer_call_fn, wte_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, ln_f_layer_call_fn while saving (showing 5 of 294). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model_gpt2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model_gpt2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('saved_model_gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For what it is; worth driving an hour out of my way for.i'm a local renter in the area and saw this place through a yelp article ranking the top indian restaurants in pinnellas. it wasn't bad but it was nothing special. i had the roti (which is a rotisserie but with a tinge of spiny melon ice cream and a good amount of spice), and the peanut pancake which was served on a bed of naan bread was the most amazingly savory and had a delicious breakfast-like syrup and peanut buttery egg white chocolatey and honey sauce. my daughter enjoyed the fried rice.  and banana pudding even though i could not a touch of which she could\n"
     ]
    }
   ],
   "source": [
    "text = \"For what it is\"\n",
    "# encoding the input text\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "# getting out output\n",
    "output = model.generate(\n",
    "  input_ids,\n",
    "  max_length = 150,\n",
    "  num_beams = 5,\n",
    "  temperature = 0.8,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=5\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The restaurant. the good: both times service was great and food is always delicious. they even have a new item (came in for chicken) which was a really nice addition. \n",
      "the bad: tiny portions, very watery atmosphere, expensive for what it is. will not return.best chicken and waffles i have ever eaten\n"
     ]
    }
   ],
   "source": [
    "text = \"The restaurant\"\n",
    "# encoding the input text\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "# getting out output\n",
    "output = model.generate(\n",
    "  input_ids,\n",
    "  max_length = 70,\n",
    "  num_beams = 5,\n",
    "  temperature = 0.7,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=5\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think they're trying too hard to be authentic and not trying to sell me stuff i don't need. i'm just glad they didn't say so (as it's probably pretty likely to stop being sold out that anything i bought is probably not worth what it is.)people of the nwc skyline that most people would like to see.  they had a great selection of beer from the vodkas (though i've never been there), and there were a few food items as well\n"
     ]
    }
   ],
   "source": [
    "text = \"I think they're\"\n",
    "# encoding the input text\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "# getting out output\n",
    "output = model.generate(\n",
    "  input_ids,\n",
    "  max_length = 100,\n",
    "  num_beams = 5,\n",
    "  temperature = 0.75,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=5\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I give up on this place (the other day i went to pick up the phone and spent a few hours there) and was told i would need to make an appointment for the next day.  as soon as i got there (a little later than i thought it would be) the young lady was unapologetic, rude, pushy, impatient, and rolled her eyes on more than one occassion that it was impossible for both of us and she didn't tell me about it either.\n"
     ]
    }
   ],
   "source": [
    "text = \"I give up on this place\"\n",
    "# encoding the input text\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "# getting out output\n",
    "output = model.generate(\n",
    "  input_ids,\n",
    "  max_length = 100,\n",
    "  num_beams = 5,\n",
    "  temperature = 0.75,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=5\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately, but it was made with dry aged ground beef with tendons and tenders in a garlic broth. i'm used to good korean food, and this was no exception. if you like lots of choices, the staff will make sure you have what you need and the menu's ready for you. the food came out in 10 minutes and was hot and delicious! i ordered the miso soup to start and it had just the right amount of kick to it! very hearty and hearty with a hearty sized bowl of chicken and veggies throughout the broth served with very deliciously spicy butternut of the best beji de-healthy flavor toasted rice and teriyaki sauce they've had (compably spicy edmonton, not even more\n"
     ]
    }
   ],
   "source": [
    "text = \"Unfortunately,\"\n",
    "# encoding the input text\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "# getting out output\n",
    "output = model.generate(\n",
    "  input_ids,\n",
    "  max_length = 150,\n",
    "  num_beams = 5,\n",
    "  temperature = 0.75,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=5\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i found the ladies and gents to be very unfriendly and unhelpful. i have been going to other department stores for years and this one was by far the best.this review is for the lunch buffet:\n",
      "i have no complaints about this restaurant and as far as the food, it certainly did not disappoint.  i ordered the mexican chicken dinner platter and while the chicken was very tasty, the portions were small and the service was absolutely terrible. when i first saw the billowing out of the $45 for dinner, i asked about the waitress, she asked if i could tell her that it was a complimentary soft drinks included gratuity was between us, which completely made no touch of like a sorry but not\n"
     ]
    }
   ],
   "source": [
    "text = \"i found the\"\n",
    "# encoding the input text\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "# getting out output\n",
    "output = model.generate(\n",
    "  input_ids,\n",
    "  max_length = 150,\n",
    "  num_beams = 5,\n",
    "  temperature = 0.75,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_return_sequences=5\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9396d7d05074184ad1bdffa89c9baf4b70aedb99cfe5c454adbba107973e00ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
